\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final, nonatbib]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[cp1251]{inputenc}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[main=russian, english]{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{subcaption}
\usepackage{algorithmicx,algpseudocode,algorithm}
\usepackage{wrapfig,tikz}
\usepackage{amsmath,longtable,fancyhdr,booktabs,multirow,graphicx,float}
\usepackage{adjustbox, bigstrut, tabularx, multirow, makecell, diagbox}
\usepackage{amssymb,xcolor,amsthm}
\usepackage{color}
\usepackage{colortbl}
\usepackage{theoremref}
\usepackage{stmaryrd}
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{caption}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs} %
\usepackage{enumitem}
\usepackage{bm}
\usepackage{placeins}
%\usepackage{natbib}

\title{Отчет по воспроизведению статьи "Generative Modeling by Estimating Gradients of the Data Distribution"}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Петр Жижин \\
  Факультет Компьютерных наук\\
  ВШЭ\\
  \texttt{piter.zh@gmail.com} \\
  % examples of more authors
   \and
   Даяна Савостьянова \\
   Факультет Компьютерных наук\\
   ВШЭ\\
   \texttt{dayanamuha@gmail.com} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Данная работа включает в себя результаты попытки воспроизведения статьи 
  "Generative Modeling by Estimating Gradients of the Data Distribution". Финальной целью является получение схожих со статьей результатов. На данный момент работа содержит проверку начальных экспериментов статьи.
  
  Работа устроена следующим образом, для начала расссмотрим статью, которая является основой для этой работы, далее рассмотрим нашу реализацию определенных частей статьи, подведем результаты о возможности аопсроизведения каждой из частей.
\end{abstract}


\section{Обзор статьи}

В обозреваемой статье предлагается вариант генеративной модели, использующей динамику Ланжевена основанную на оценке градиентов распределения данных с помощью score matching. В качестве архитектуры в модели используется модель RefineNet из ~\cite{DBLP:journals/corr/LinMS016} с модификацией функции активации (ELU).

\subsection{Модель}

В качестве архитектуры в модели используется модель RefineNet из ~\cite{DBLP:journals/corr/LinMS016} с модификацией функции активации (ELU),


\subsection{Лосс}

Для сэмплирования из распределения данных $p_{data}(x)$ при помощи алгоритма
Ланжевена, необходимо уметь строить оценку на градиент
$s_{\theta}(x) \approx \nabla_x \log p_{data}(x)$.

Если бы у нас была известна плотность, то её аппроксимацию можно было бы
посчитать, прооптимизировав следующий функционал:

\[
\mathbb{E}_{p_{data}(x)} \left[ | s_{\theta}(x) - \nabla_x \log p_{data}(x) |_2^2 \right] \to \min_{\theta}
\]

Однако на практике мы не можем знать плотность распределения, а знаем лишь
только некоторую выборку, которая была сгенерирована по данным.

В статье~\cite{sliced_score_matching} было показано, что эту формулу можно так
же представить и в другом виде, который можно использовать для подсчёта.

Мы будем использовать две переформулировки данного уравнения в разных задачах.
Первая из них называется Sliced Score Matching: 

\begin{equation}
    \mathbb{E}_{p_{data}(x)} \mathbb{E}_{p_v} \left[ v^T \nabla_x s_{\theta}(x) v + \frac{1}{2} |s_{\theta}(x)|_2^2 \right]
    \label{eq:sliced_score_matching}
\end{equation}

Для подсчёта этой формулы, мы генерируем случайные вектора $v$. Например, это могут быть стандартные
гауссовские вектора.

$\nabla_x s_{\theta}(x)$ --- это гессиан логарифма плотности распределений. Так как напрямую
посчитать гессиан для любой достаточно большой модели было бы практически невозможно, то
он напрямую не считается. В той же статье было показано, как при помощи любой библиотеки автоматического
подсчёта градиента можно посчитать этот лосс, не считая при этом гессиан.

Вторая называется Denoising score matching:
\begin{equation}
    \mathbb{E}_{q(\tilde{x} | x)p_{data}(x)} \left[ |s_{\theta}(\tilde{x}) - \nabla_x \log q_{\sigma}(\tilde{x} | x)|_2^2 \right]
    \label{eq:denoising_score_matching}
\end{equation}

Тут суть в том, чтобы в данные добавлялся шум $q(\tilde{x} | x)$ с известной
плотностью, матожиданием равным $x$ и низкой дисперсией.


\section{Эксперимент на смеси гауссовских распределений}

\subsection{Неверная оценка градиента правдоподобия выборки}
Повторим эксперимент из статьи, чтобы показать, что Score Matching лосс адекватно
оценивает градиент только в областях высокой плотности. В качестве модели был
использован MLP (Multi-Layer Perceptron) с тремя слоями. Размером скрытого слоя
128, функция активации Softplus. Данные генерировались из смеси распределений:
\[
\frac{1}{5} \mathcal{N}\left( \left( -5, -5\right), I \right)+\frac{4}{5} \mathcal{N}\left( \left( 5, 5\right), I \right)
\]

В качестве оптимизатора был взят Adam с lr = 0.001 с размером батча 128. 
Модель училась 10000 итераций.

В качестве лосса для Score Matching тут использовался Sliced Score Matching с
генерацией только одного случайного вектора $v$ для каждого элемента выборки.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
  	\includegraphics[width=0.95\textwidth]{fig2a}
  	\caption{$\nabla_x \log p_{data}(x)$ --- истинный градиент логарифма правдоподобия}
  	\label{fig:2a}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{fig2b}
    \caption{$s_{\theta}(x)$ --- оценка на $\nabla_x \log p_{data}(x)$}
    \label{fig:2b}
  \end{subfigure}
  \caption{Сравнение градиента истинного логарифма правдоподобия с его оценкой
  MLP сетью, красные области соответствуют области с высоким значеием плотности
  рассматриваемого распределения}
  \label{fig:2}
\end{figure}

На Рис.~\ref{fig:2} мы можем видеть, что нейронная сеть адекватно оценивает
градиент $\nabla_x \log p_{data}(x)$ только в области вокруг мод распределения.
Стрелки указывают в то же направление, что и настоящий градиент плотности.

Значения ожидаемо отличаются в области, где плотность мала.

Стоит отметить, что именно поэтому некорректно было бы использовать обычный
алгоритм Ланжевена. Плотность рассматриваемого распределения везде ненулевая.
Однако расстояние между двумя модами сильно большое.

Данный результат полностью сходится с тем, что было получено в статье.

\subsection{Динамика Ланжевена}

Хотим посмотреть на разницу в работе обычного Ланжевена и Ланжевена с отжигом. В алгоритме будем использовать для подсчета честные градиенты. Для этого генерировать данные будем как в предыдущем пункте из смеси Гауссовских распределений. Рисунок~\ref{fig:3a} В качестве начальных точек для обеих динамик будем брать равномерно выбранные токи из квадрата $[-8, 8] \times [-8, 8]$.  Для обычного Ланжевена возьмем  количество шагов 1000, $\varepsilon=0.1$. Рисунок~\ref{fig:3b}. Для Ланжевена с отжигом ~\ref{alg:anneal} 100, $\varepsilon=0.1$. В статье предлагается взять $\sigma_i$ из геометрической прогрессии, где $\sigma_{1} = 10, \sigma_{10} = 0.1$. Отметим, что при данных параметрах предложенный  в статье алгоритм динамики Ланжевена с отжигом не сходится. После некоторых экспериментов с параметрами пришли к следующим параметрам:
$\sigma_i$ из геометрической прогрессии, где $\sigma_{1} = 20, \sigma_{10} = 0.7$. Рисунок~\ref{fig:3c}.

Заметим, что обычный Ланжевен плохо угадывает доли событий по компонентам смеси. С другой стороны Ланжевен с отжигом смог уловить соотношение между компонентами. Отметим, что качество работы последнего метода зависят от выбора сигмы. Метод может как не сойтись, так и выдавать результаты в той же мере неверные, как и обычный Ланжевен.


\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{fig3a}
		\caption{Сэмплирование}
		\label{fig:3a}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{fig3b}
		\caption{Динамика Ланжевина}
		\label{fig:3b}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{fig3c}
		\caption{Динамика Ланжевена с отжига}
		\label{fig:3c}
	\end{subfigure}
\end{figure}

Данный результат в целом сходится с тем, что было получено в статье, но стоит отметить, что параметры в статье указаны неподходящие.

\subsection{Figures}

\section{Реализации на данный момент}

Так же кроме предыдущих экспериментов были реализованы методы динамик Ланжевена для генерации картинок.

\begin{subfigure}{scale=1}
	\includegraphics[width=\textwidth]{fig3a}
	\caption{Сэмплирование}
	\label{fig:3a}
\end{subfigure}


%\section*{References}

\bibliographystyle{unsrt} 
\bibliography{rev} 

\section{Appendix}

\begin{wrapfigure}[15]{r}{0.5\textwidth}
	\vspace{-2.2em}
	\begin{minipage}{0.5\textwidth}
		\begin{algorithm}[H]
			\caption{Annealed Langevin dynamics.}
			\label{alg:anneal}
			\begin{algorithmic}[1]
				\Require{$\{\sigma_i\}_{i=1}^L, \epsilon, T$.}
				\State{Initialize $\tilde{\bfx}_0$}
				\For{$i \gets 1$ to $L$}
				\State{$\alpha_i \gets \epsilon \cdot \sigma_i^2/\sigma_L^2$} \Comment{$\alpha_i$ is the step size.}
				\For{$t \gets 1$ to $T$}
				\State{Draw $\bfz_t \sim \mcal{N}(0, I)$}
				\State{\resizebox{0.75\textwidth}{!}{$\tilde{\bfx}_{t} \gets \tilde{\bfx}_{t-1} + \dfrac{\alpha_i}{2} \bfs_\bftheta(\tilde{\bfx}_{t-1}, \sigma_i) + \sqrt{\alpha_i}~ \bfz_t$}}
				\EndFor
				\State{$\tilde{\bfx}_0 \gets \tilde{\bfx}_T$}
				\EndFor
				\item[]
				\Return{$\tilde{\bfx}_T$}
			\end{algorithmic}
		\end{algorithm}
	\end{minipage}
\end{wrapfigure}


\end{document}
