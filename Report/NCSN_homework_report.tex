\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final, nonatbib]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[cp1251]{inputenc}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[main=russian, english]{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{subcaption}
%\usepackage{natbib}

\title{Отчет по воспроизведению статьи "Generative Modeling by Estimating Gradients of the Data Distribution"}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Петр Жижин \\
  Факультет Компьютерных наук\\
  ВШЭ\\
  \texttt{piter.zh@gmail.com} \\
  % examples of more authors
   \and
   Даяна Савостьянова \\
   Факультет Компьютерных наук\\
   ВШЭ\\
   \texttt{dayanamuha@gmail.com} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Данная работа включает в себя результаты попытки воспроизведения статьи 
  "Generative Modeling by Estimating Gradients of the Data Distribution". Финальной целью является получение схожих со статьей результатов. На данный момент работа содержит проверку начальных экспериментов статьи.
  
  Работа устроена следующим образом, для начала расссмотрим статью, которая является основой для этой работы, далее рассмотрим нашу реализацию определенных частей статьи, подведем результаты о возможности аопсроизведения каждой из частей.
\end{abstract}


\section{Обзор статьи}

В обозреваемой статье предлагается вариант генеративной модели, использующей динамику Ланжевена основанную на оценке градиентов распределения данных с помощью score matching. В качестве архитектуры в модели используется модель RefineNet из ~\cite{DBLP:journals/corr/LinMS016} с модификацией функции активации (ELU).

\subsection{Модель}

\subsection{Лосс}

Для сэмплирования из распределения данных $p_{data}(x)$ при помощи алгоритма
Ланжевена, необходимо уметь строить оценку на градиент
$s_{\theta}(x) \approx \nabla_x \log p_{data}(x)$.

Если бы у нас была известна плотность, то её аппроксимацию можно было бы
посчитать, прооптимизировав следующий функционал:

\[
\mathbb{E}_{p_{data}(x)} \left[ | s_{\theta}(x) - \nabla_x \log p_{data}(x) |_2^2 \right] \to \min_{\theta}
\]

Однако на практике мы не можем знать плотность распределения, а знаем лишь
только некоторую выборку, которая была сгенерирована по данным.

В статье~\cite{sliced_score_matching} было показано, что эту формулу можно так
же представить и в другом виде, который можно использовать для подсчёта.

Мы будем использовать две переформулировки данного уравнения в разных задачах.
Первая из них называется Sliced Score Matching: 

\begin{equation}
    \mathbb{E}_{p_{data}(x)} \mathbb{E}_{p_v} \left[ v^T \nabla_x s_{\theta}(x) v + \frac{1}{2} |s_{\theta}(x)|_2^2 \right]
    \label{eq:sliced_score_matching}
\end{equation}

Для подсчёта этой формулы, мы генерируем случайные вектора $v$. Например, это могут быть стандартные
гауссовские вектора.

$\nabla_x s_{\theta}(x)$ --- это гессиан логарифма плотности распределений. Так как напрямую
посчитать гессиан для любой достаточно большой модели было бы практически невозможно, то
он напрямую не считается. В той же статье было показано, как при помощи любой библиотеки автоматического
подсчёта градиента можно посчитать этот лосс, не считая при этом гессиан.

Вторая называется Denoising score matching:
\begin{equation}
    \mathbb{E}_{q(\tilde{x} | x)p_{data}(x)} \left[ |s_{\theta}(\tilde{x}) - \nabla_x \log q_{\sigma}(\tilde{x} | x)|_2^2 \right]
    \label{eq:denoising_score_matching}
\end{equation}

Тут суть в том, чтобы в данные добавлялся шум $q(\tilde{x} | x)$ с известной
плотностью, матожиданием равным $x$ и низкой дисперсией.


\section{Эксперимент на смеси гауссовских распределений}

\subsection{Неверная оценка градиента правдоподобия выборки}
Повторим эксперимент из статьи, чтобы показать, что Score Matching лосс адекватно
оценивает градиент только в областях высокой плотности. В качестве модели был
использован MLP (Multi-Layer Perceptron) с тремя слоями. Размером скрытого слоя
128, функция активации Softplus. Данные генерировались из смеси распределений:
\[
\frac{1}{5} \mathcal{N}\left( \left( -5, -5\right), I \right)+\frac{4}{5} \mathcal{N}\left( \left( 5, 5\right), I \right)
\]

В качестве оптимизатора был взят Adam с lr = 0.001 с размером батча 128. 
Модель училась 10000 итераций.

В качестве лосса для Score Matching тут использовался Sliced Score Matching с
генерацией только одного случайного вектора $v$ для каждого элемента выборки.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
  	\includegraphics[width=0.95\textwidth]{fig2a}
  	\caption{$\nabla_x \log p_{data}(x)$ --- истинный градиент логарифма правдоподобия}
  	\label{fig:2a}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{fig2b}
    \caption{$s_{\theta}(x)$ --- оценка на $\nabla_x \log p_{data}(x)$}
    \label{fig:2b}
  \end{subfigure}
  \caption{Сравнение градиента истинного логарифма правдоподобия с его оценкой
  MLP сетью, красные области соответствуют области с высоким значеием плотности
  рассматриваемого распределения}
  \label{fig:2}
\end{figure}

На Рис.~\ref{fig:2} мы можем видеть, что нейронная сеть адекватно оценивает
градиент $\nabla_x \log p_{data}(x)$ только в области вокруг мод распределения.
Стрелки указывают в то же направление, что и настоящий градиент плотности.

Значения ожидаемо отличаются в области, где плотность мала.

Стоит отметить, что именно поэтому некорректно было бы использовать обычный
алгоритм Ланжевена. Плотность рассматриваемого распределения везде ненулевая.
Однако расстояние между двумя модами сильно большое.

Данный результат полностью сходится с тем, что было получено в статье.

\subsection{Toy Experiment №2}










\subsection{Figures}

%\begin{figure}
%  \centering
%  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%  \caption{Sample figure caption.}
%\end{figure}

All artwork must be neat, clean, and legible. Lines should be dark enough for
purposes of reproduction. The figure number and caption always appear after the
figure. Place one line space before the figure caption and one line space after
the figure. The figure caption should be lower case (except for first word and
proper nouns); figures are numbered consecutively.

You may use color figures.  However, it is best for the figure captions and the
paper body to be legible if the paper is printed in either black/white or in
color.


%\section*{References}

\bibliographystyle{unsrt} 
\bibliography{rev} 

\end{document}
